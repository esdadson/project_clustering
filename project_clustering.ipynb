{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06a88cc",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "## DSC495-015 Scientific Programming with Python\n",
    "### Contributer: Evan Dadson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3755f",
   "metadata": {},
   "source": [
    "### Project Description\n",
    "For this project I aim to \n",
    "\n",
    "* (a) implement a clustering algorithm of my choice, and \n",
    "* (b) compare my implementation on 1 to 2 datasets.\n",
    "\n",
    "### Algorithm Choice: K-means\n",
    "\n",
    "The K-means algorithm is known for it's simplicity and can be broken down into three steps. \n",
    "\n",
    "* Choose the amount of centroids ($k$) to create and starting data points for the centroids.\n",
    "* Assign each data point to the closest centroid.\n",
    "* Reassign the centroid as the mean of data points assigned to centroid $i$ in $k$.\n",
    "\n",
    "The algorithm repeats the last two steps until a maximum amount of iterations has been reached or until the centroid means move by less than some specified threshold value (e.g. 1e-4). For my implementation, I will specify a maximum amount of iterations but not a threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a4d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5047ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define euclidean distances for assigning data points to centroids\n",
    "def euclid_distances(a,b, **kwargs):\n",
    "    \"\"\"\n",
    "    Definition for computing the euclidean distance between two points.\n",
    "    input: a,b two data points of size n\n",
    "    output: distances between points\n",
    "    \"\"\"\n",
    "    distances = np.linalg.norm(a-b, axis=1)\n",
    "    return distances  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964f5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My implementation of the KMeans algorithm using iterative assignment of data vectors to clusters.\n",
    "class KMean_dadson:\n",
    "    \"\"\"\n",
    "    The KMean_dadson class implements the K-means algorithm on a set of data vectors.\n",
    "    \n",
    "    Attributes\n",
    "        ~ n_clusters: chosen number of clusters to use in algorithm.\n",
    "        ~ max_iter: maximum amount of updates to be made to the starting centroids.\n",
    "        \n",
    "    Methods:\n",
    "        random_init(data,n_clusters)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, max_iter = 300):\n",
    "        \"\"\"\n",
    "        This is the constructor for the class.\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter=max_iter\n",
    "        \n",
    "    def random_init(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Randomly assign starting centroids to points in the dataset.\n",
    "        Input: data of size m x n\n",
    "        input: **kwargs is any other input\n",
    "        Return: initialized centroids\n",
    "        \"\"\"\n",
    "        # dimensions of data vectors m x n\n",
    "        m,n = data.shape\n",
    "        \n",
    "        # initialize array to store k centroids of size n\n",
    "        centroids = np.empty([self.n_clusters,n])\n",
    "        \n",
    "        # assign centroids by picking random data vectors\n",
    "        for k in range(self.n_clusters):\n",
    "            random_int = np.random.randint(m)\n",
    "            centroids[k] = data[random_int,:]\n",
    "            \n",
    "        return centroids\n",
    "    \n",
    "    def assignment(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Assingns the data points to the closest centroid and updates the centroids until max iteration.\n",
    "        input: data of size m x n\n",
    "        input: **kwargs is any other input\n",
    "        return: centroids array of k x n data vectors\n",
    "        return: clusters array of size m with classifications\n",
    "        \"\"\"\n",
    "        # dimensions of the data stored for use\n",
    "        m,n = data.shape\n",
    "        \n",
    "        # use the random_init method to randomly initialize the starting centroids\n",
    "        centroids = self.random_init(data)\n",
    "        \n",
    "        # define clusters to be an array of size m\n",
    "        clusters = np.zeros(m)\n",
    "        \n",
    "        # define distances to be an array of size m x k\n",
    "        distances = np.zeros([m,self.n_clusters])\n",
    "        \n",
    "        # iterate over maximum number of iterations\n",
    "        for i in range(self.max_iter):\n",
    "            \n",
    "            # take distances from data points to centroids\n",
    "            for k in range(self.n_clusters):\n",
    "                distances[:,k] = euclid_distances(centroids[k],data)\n",
    "            \n",
    "            # assigns data points to a cluster defined by the index of the closest cluster\n",
    "            clusters = np.argmin(distances,axis=1)\n",
    "            \n",
    "            # reassign the centroids to be the mean of data points in each cluster\n",
    "            for k in range(self.n_clusters):\n",
    "                centroids[k] = np.mean(data[clusters == k],0)\n",
    "                \n",
    "        return (centroids,clusters)\n",
    "    \n",
    "    def __call__(self,data):\n",
    "        \"\"\"\n",
    "        Allows us to run the assignment method without calling the method.\n",
    "        \"\"\"\n",
    "        return self.assignment(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8adbbf",
   "metadata": {},
   "source": [
    "### Testing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4750860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "The number of data vectors determined to be 100 but is actually 100\n",
      "The absolute error is 0\n",
      "\n",
      "\n",
      "Test 2\n",
      "The dimension of the data is determined to be 2 but is actually 2\n",
      "The absolute error is 0\n",
      "\n",
      "\n",
      "Test 3\n",
      "The number of data vectors determined to be 346 but is actually 346\n",
      "The absolute error is 0\n",
      "\n",
      "\n",
      "Test 4\n",
      "The dimension of the data is determined to be 5 but is actually 5\n",
      "The absolute error is 0\n",
      "\n",
      "\n",
      "Test 5\n",
      "The number of clusters is determined to be 4 but is actually 4\n",
      "The absolute error is 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate data for testing implementation\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def test_Kmeans():\n",
    "    # tests that KMean_dadson catches the correct number of data vectors\n",
    "    X, y = make_blobs(n_samples=100, centers=3, n_features=2)\n",
    "    kmeans = KMean_dadson(n_clusters = 3, max_iter = 300)\n",
    "    ATrue = 100\n",
    "    AComp = len(kmeans(X)[1])\n",
    "    print(\"Test 1\")\n",
    "    print(\"The number of data vectors determined to be {0} but is actually {1}\".format(AComp,ATrue))\n",
    "    print(\"The absolute error is %0.15g\" % (abs(ATrue - AComp)))\n",
    "    print('\\n')\n",
    "    \n",
    "    # tests that KMean_dadson catches the correct size of each data vector\n",
    "    X, y = make_blobs(n_samples=100, centers=3, n_features=2)\n",
    "    kmeans = KMean_dadson(n_clusters = 3, max_iter = 300)\n",
    "    AComp = len(kmeans(X)[0][0])\n",
    "    ATrue = 2\n",
    "    print(\"Test 2\")\n",
    "    print(\"The dimension of the data is determined to be {0} but is actually {1}\".format(AComp,ATrue))\n",
    "    print(\"The absolute error is %0.15g\" % (abs(ATrue - AComp)))\n",
    "    print('\\n')\n",
    "    \n",
    "    # tests that KMean_dadson catches the correct number of data vectors\n",
    "    X, y = make_blobs(n_samples=346, centers=4, n_features=5)\n",
    "    kmeans = KMean_dadson(n_clusters = 4, max_iter = 300)\n",
    "    ATrue = 346\n",
    "    AComp = len(kmeans(X)[1])\n",
    "    print(\"Test 3\")\n",
    "    print(\"The number of data vectors determined to be {0} but is actually {1}\".format(AComp,ATrue))\n",
    "    print(\"The absolute error is %0.15g\" % (abs(ATrue - AComp)))\n",
    "    print('\\n')\n",
    "    \n",
    "    # tests that KMean_dadson catches the correct size of each data vector\n",
    "    X, y = make_blobs(n_samples=346, centers=4, n_features=5)\n",
    "    kmeans = KMean_dadson(n_clusters = 4, max_iter = 300)\n",
    "    AComp = len(kmeans(X)[0][0])\n",
    "    ATrue = 5\n",
    "    print(\"Test 4\")\n",
    "    print(\"The dimension of the data is determined to be {0} but is actually {1}\".format(AComp,ATrue))\n",
    "    print(\"The absolute error is %0.15g\" % (abs(ATrue - AComp)))\n",
    "    print('\\n')\n",
    "    \n",
    "    # tests that k unique targets are created \n",
    "    X, y = make_blobs(n_samples=346, centers=4, n_features=5)\n",
    "    kmeans = KMean_dadson(n_clusters = 4, max_iter = 300)\n",
    "    AComp = len(kmeans(X)[0])\n",
    "    ATrue = 4\n",
    "    print(\"Test 5\")\n",
    "    print(\"The number of clusters is determined to be {0} but is actually {1}\".format(AComp,ATrue))\n",
    "    print(\"The absolute error is %0.15g\" % (abs(ATrue - AComp)))\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "test_Kmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e1eace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA of Iris dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9815b1",
   "metadata": {},
   "source": [
    "#### Analysis of Iris dataset\n",
    "There are four features used to predict the species of iris: \n",
    "* sepal length (cm)\n",
    "* sepal width (cm)\n",
    "* petal length (cm)\n",
    "* petal width (cm)\n",
    "\n",
    "The flower species:\n",
    "* iris setosa\n",
    "* iris versicolour\n",
    "* iris virginica\n",
    "\n",
    "The purpose of running a clustering algorithm on the dataset is to determine which features constitute a different species of iris. Hence, the amount of clusters is not known in prior. Thus, the first task is to choose the optimal amount of clusters. A good way to do such that is by using the elbow method. The elbow method plots the overall cluster coherence, i.e. the sum of the within cluster coherences, for each value of k in a specified range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d8a5107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 150 data vectors of dimension 4.\n"
     ]
    }
   ],
   "source": [
    "data = iris.data\n",
    "target = iris.target\n",
    "print(\"There are {0} data vectors of dimension {1}.\".format(data.shape[0],data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b879486",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "I will first use the elbow method with KMeans algorithm from sklearn.cluster to\n",
    "determine the optimal number of clusters to use in the algorithm. \n",
    "\n",
    "The method consists of plotting the overall cluster coherence, i.e. inertia, for each \n",
    "choice of k number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f50ea039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Finding the optimum number of clusters for k-means classification\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "coherence = []\n",
    "\n",
    "for k in range(1,7):\n",
    "    kmeans = KMeans(n_clusters = k, init = 'random', max_iter = 300)\n",
    "    kmeans.fit(data)\n",
    "    coherence.append(kmeans.inertia_)\n",
    "    \n",
    "%matplotlib inline\n",
    "x = np.linspace(1,6,6)\n",
    "y = coherence\n",
    "plt.plot(x,y)\n",
    "plt.title(\"Elbow Method of Iris Dataset\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Overall Cluster Coherence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0063838",
   "metadata": {},
   "source": [
    "#### Cluster Choice\n",
    "The optimal amount of clusters for a given dataset as determined by the elbow method is given by the value of k where the last noticible elbow appears on the plot. As you can see, the last notable elbow appears at k = 3. Therefore, 3 is the optimal amount of clusters to use for this dataset. Anything over 3 clusters would overfit the data and not generate very useful classifications of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "addbf74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n",
      "[[5.9016129  2.7483871  4.39354839 1.43387097]\n",
      " [5.006      3.428      1.462      0.246     ]\n",
      " [6.85       3.07368421 5.74210526 2.07105263]]\n"
     ]
    }
   ],
   "source": [
    "# Implementing sklearn.cluster.KMeans on Iris\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300).fit(data)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6d1ae5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.85       3.07368421 5.74210526 2.07105263]\n",
      " [5.9016129  2.7483871  4.39354839 1.43387097]\n",
      " [5.006      3.428      1.462      0.246     ]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Implementing my kmeans clustering on Iris\n",
    "kmeans2 = KMean_dadson(n_clusters = 3, max_iter = 300)\n",
    "print(kmeans2(data)[0])\n",
    "print(kmeans2(data)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "adf38626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f1faec",
   "metadata": {},
   "source": [
    "### Contingency Table for measuring accuracy\n",
    "\n",
    "For this I used sklearn.metrics.cluser.pair_confusion_matrix to determine the confusion matrix from my implementation of the K-means model to the target values from the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f62f8e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10264  1490]\n",
      " [ 4736  5860]]\n"
     ]
    }
   ],
   "source": [
    "# import the contingency table from sklearn.metrics.cluster\n",
    "from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "predicted = kmeans2(data)[1]\n",
    "actual = target\n",
    "print(pair_confusion_matrix(predicted,actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f043f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, my implementation of the K-means algorithm did not work all that well in clustering the data points. This can be seen by the non-zero values on the off diagnonal in the confusion matrix. \n",
    "\n",
    "Although, as seen in the implementation of my clustering algorithm on the iris dataset as compared to the actual target values, the clusters seem to pair well with the target values generated by the iris dataset.\n",
    "\n",
    "There are a few reasons why the clustering may not perform extremely well. First, in K-means clustering the indexes of the clusters do not necessarily hold valuable information other than what groupings the data belong to. For example, a value of 1 in one clustering does not necessarily hold true for a value of 1 in another clustering. If I were to do this again, I would want to take that into account in my code. Additionally, selecting the starting centroids at random may not produce as good as results, whereas using a k-means++ initialization scheme to initialize the points may generate much better results and converge much faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
